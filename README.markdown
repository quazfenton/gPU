# Kaggle Notebook -> Serverless API Toolkit

This project lets you pull/upload a Kaggle notebook, run it on Kaggle, and deploy it as a serverless API with Google Cloud Functions (and package for AWS Lambda). It also includes helpers for local serving and calling your deployed endpoint.

Highlights
- Auth fallback: env -> ~/.kaggle/kaggle.json -> .env -> kagglehub
- Pull/Push/Run with Kaggle CLI (KaggleApi optional)
- Convert .ipynb to script via nbconvert
- Package and deploy to GCF with a consistent Flask entrypoint predict_handler
- Optional model integration via deploy_model.py with auto requirements
- Package for AWS Lambda (zip) and local dev server support (functions-framework)
- Endpoint registry and “call” command to easily hit your function

Quickstart
1) Install dependencies
   pip install -r requirements.txt

2) Set Kaggle credentials
   - Either set env vars KAGGLE_USERNAME and KAGGLE_KEY
   - Or put kaggle.json at ~/.kaggle/kaggle.json
   - Or create a .env in this folder with KAGGLE_USERNAME and KAGGLE_KEY

3) Validate
   python kaggle.py --validate
   # Expect: Setup validation passed!

4) Run and deploy in one go (example)
   python kaggle.py run USER/NOTEBOOK --deploy --gcp-project YOUR_PROJECT_ID --save-name my-func

5) Call your function later
   - List stored endpoints: python kaggle.py endpoints
   - Call by name: python kaggle.py call my-func --json '{"features": [[1,2,3]]}'

Commands
- List kernels: python kaggle.py list [--user USER]
- Pull kernel: python kaggle.py pull USER/NOTEBOOK [--dest DIR]
- Create kernel from local/remote:
  - python kaggle.py create /abs/or/rel/path/to/notebook.ipynb
  - python kaggle.py create https://github.com/owner/repo/blob/main/path/to/nb.ipynb
- Run kernel (and optionally deploy):
  - python kaggle.py run USER/NOTEBOOK --deploy [GCP OPTIONS]
  - python kaggle.py run https://kaggle.com/code/user/notebook --deploy [GCP OPTIONS]
  - python kaggle.py run /path/to/notebook.ipynb --deploy [GCP OPTIONS]

GCP options (usable with run --deploy and deploy)
- --gcp-project YOUR_PROJECT_ID
- --region us-central1 (default)
- --function-name NAME (default autogenerated)
- --memory 512MB (default)
- --timeout 540s (default)
- --public (default) or --private
- --save-name friendly name for the endpoint registry

Deploy existing dir
- python kaggle.py deploy ./dir-with-ipynb [GCP OPTIONS]

AWS packaging
- python kaggle.py package-aws ./dir-with-ipynb
  # Produces aws-lambda.zip and aws-lambda/ with lambda_function.py
  # Example AWS CLI deploy:
  # aws lambda create-function --function-name my-func \
  #   --runtime python3.11 \
  #   --zip-file fileb://aws-lambda.zip \
  #   --handler lambda_function.handler \
  #   --role arn:aws:iam::<ACCOUNT_ID>:role/<ROLE_NAME>

Local serving (dev/test)
- Package and run locally:
  python kaggle.py serve-local ./dir-with-ipynb --run --port 8080
- Then send a request:
  curl -X POST -H 'Content-Type: application/json' \
       -d '{"features": [[1,2,3]]}' http://localhost:8080/

Endpoint registry and calling
- Save-name during deployment stores the function URL (if parsed) in .kaggle_state/endpoints.json
- List: python kaggle.py endpoints
- Call:
  - python kaggle.py call my-func --json '{"features": [[1,2,3]]}'
  - python kaggle.py call https://your-func-url --json-file payload.json

Model integration
- If deploy_model.py exists in this folder or the notebook folder:
  - It’s included during packaging for GCF/AWS
  - The generated handler will try notebook process_request first, then dispatch to deploy_model.predict(features), else echo input
  - requirements.txt generated under deploy/ (or aws-lambda/) will include basic libraries auto-detected from imports in deploy_model.py (torch, scikit-learn, numpy, pandas, requests, kagglehub, huggingface_hub, python-dotenv)
- Be aware of cold start and size limits when including heavy ML frameworks. Consider Cloud Run or Lambda container images if needed.

Troubleshooting
- gcloud --source errors: We now run gcloud with cwd=deploy/ and use --source .
- Credentials: If .env isn’t loading in your shell, the script loads it directly with override=True.
- KaggleApi shadowing: If you plan to import kaggle.* modules from Python code in this directory, rename kaggle.py to avoid shadowing the package.
- Check environment:
  python kaggle.py doctor

Security
- Never commit your secrets. The CLI uses environment variables and loads .env only locally.
- For runtime secrets on GCF/AWS, prefer secret managers or environment variables set in the platform.
    
    # If credentials are still not found, raise error
    if not username or not key:
        raise RuntimeError("KAGGLE_USERNAME/KAGGLE_KEY not set in environment or .env file")
    
    return (username, key)

# ----------------------------
# HTTP Wrappers
# ----------------------------

def kaggle_get(path, params=None, stream=False):
    url = f"{BASE_URL}{path}"
    r = requests.get(url, auth=get_auth(), params=params, stream=stream)
    if r.status_code != 200:
        raise RuntimeError(f"GET {url} failed: {r.status_code} {r.text}")
    return r

def kaggle_post(path, data=None, files=None):
    url = f"{BASE_URL}{path}"
    r = requests.post(url, auth=get_auth(), json=data, files=files)
    if r.status_code not in (200, 201):
        raise RuntimeError(f"POST {url} failed: {r.status_code} {r.text}")
    return r

# ----------------------------
# Kernels API
# ----------------------------

def kernels_list(username=None):
    params = {"user": username} if username else {}
    r = kaggle_get("/kernels/list", params=params)
    return r.json()

def kernel_pull(ref, dest="."):
    """Download kernel source (notebook files)."""
    r = kaggle_get("/kernels/pull", params={"kernel": ref}, stream=True)
    out = os.path.join(dest, f"{ref.replace('/','_')}.zip")
    with open(out, "wb") as f:
        for chunk in r.iter_content(chunk_size=8192):
            f.write(chunk)
    logger.info(f"Pulled kernel → {out}")
    return out

def kernel_push(path):
    """Push a new kernel version (expects kernel-metadata.json inside path)."""
    meta_path = os.path.join(path, "kernel-metadata.json")
    if not os.path.exists(meta_path):
        raise RuntimeError("kernel-metadata.json missing in path")

    with open(meta_path) as f:
        metadata = json.load(f)

    files = {}
    for fn in os.listdir(path):
        if fn.endswith(".ipynb") or fn == "kernel-metadata.json":
            files[fn] = open(os.path.join(path, fn), "rb")

    r = kaggle_post("/kernels/push", files=files)
    logger.info(f"Kernel push response: {r.text}")
    return r.json()

def kernel_status(ref):
    r = kaggle_get("/kernels/status", params={"kernel": ref})
    return r.json()

def kernel_output(ref, dest="."):
    r = kaggle_get("/kernels/output", params={"kernel": ref}, stream=True)
    out = os.path.join(dest, f"{ref.replace('/','_')}_output.zip")
    with open(out, "wb") as f:
        for chunk in r.iter_content(chunk_size=8192):
            f.write(chunk)
    logger.info(f"Downloaded kernel output → {out}")
    return out

def kernels_files(ref):
    r = kaggle_get("/kernels/files", params={"kernel": ref})
    return r.json()

# ----------------------------
# Models API
# ----------------------------

def get_model_instance(owner, model, framework, instance):
    r = kaggle_get(f"/models/{owner}/{model}/{framework}/{instance}/get")
    return r.json()

# ----------------------------
# Commands
# ----------------------------

def create_notebook(args):
    file_path = args.file
    if not file_path or file_path in (".", "./"):
        # fallback to first .ipynb in cwd
        ipynbs = [f for f in os.listdir(".") if f.endswith(".ipynb")]
        if not ipynbs:
            logger.error("No .ipynb found")
            return
        file_path = ipynbs[0]

    if not os.path.exists(file_path):
        logger.error(f"Notebook file not found: {file_path}")
        return

    with tempfile.TemporaryDirectory() as tmp:
        nb_name = os.path.basename(file_path)
        shutil.copy(file_path, os.path.join(tmp, nb_name))

        metadata = {
            "title": os.path.splitext(nb_name)[0].replace("_", " ").title(),
            "id": f"{os.environ.get('KAGGLE_USERNAME','user')}/{nb_name.lower().replace('.ipynb','')}",
            "code_file": nb_name,
            "language": "python",
            "kernel_type": "notebook",
            "dataset_sources": args.datasets.split(",") if args.datasets else []
        }
        with open(os.path.join(tmp, "kernel-metadata.json"), "w") as f:
            json.dump(metadata, f, indent=2)

        kernel_push(tmp)

def run_notebook(args):
    ref = args.notebook
    if not ref:
        kernels = kernels_list(os.environ.get("KAGGLE_USERNAME"))
        if not kernels:
            logger.error("No kernels available")
            return
        for i, k in enumerate(kernels, 1):
            print(f"{i}. {k['ref']} ({k['title']})")
        choice = int(input("Choose: "))
        ref = kernels[choice-1]['ref']

    # Pull → Push flow
    out = kernel_pull(ref)
    with tempfile.TemporaryDirectory() as tmp:
        shutil.unpack_archive(out, tmp)
        kernel_push(tmp)

def list_kernels(args):
    kernels = kernels_list(os.environ.get("KAGGLE_USERNAME"))
    for k in kernels:
        print(f"{k['ref']} - {k['title']}")

def get_model_cmd(args):
    data = get_model_instance(args.owner, args.model, args.framework, args.instance)
    print(json.dumps(data, indent=2))

# ----------------------------
# CLI
# ----------------------------

def main():
    parser = argparse.ArgumentParser("Kaggle API CLI")
    sub = parser.add_subparsers(dest="cmd")

    p = sub.add_parser("create")
    p.add_argument("--file")
    p.add_argument("--datasets")
    p.set_defaults(func=create_notebook)

    p = sub.add_parser("run")
    p.add_argument("--notebook")
    p.set_defaults(func=run_notebook)

    p = sub.add_parser("list")
    p.set_defaults(func=list_kernels)

    p = sub.add_parser("model")
    p.add_argument("owner")
    p.add_argument("model")
    p.add_argument("framework")
    p.add_argument("instance")
    p.set_defaults(func=get_model_cmd)

    args = parser.parse_args()
    if not args.cmd:
        parser.print_help()
        return
    args.func(args)

if __name__ == "__main__":
    main()
