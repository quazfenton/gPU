"""
Google Cloud Function generated from Kaggle notebook.
"""
import json
import logging
from flask import Flask, request, jsonify

# Optional model import if available
try:
    import deploy_model as _deploy_model
    _HAS_MODEL = True
except Exception:
    _deploy_model = None
    _HAS_MODEL = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Notebook code
#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# In[ ]:


#!/usr/bin/env python
# coding: utf-8

# # DeepSeek-R1-0528-Qwen3-8B: Complete Production Implementation
# 
# This notebook provides a comprehensive implementation of the DeepSeek-R1-0528-Qwen3-8B model with advanced features for production deployment on Kaggle.

# ## Initial Setup and Package Installation

import subprocess
import sys

def install_packages():
    """Install required packages with proper error handling"""
    packages = [
        "transformers>=4.52.0",
        "bitsandbytes>=0.46.0",
        "accelerate",
        "torch",
        "ipywidgets",
        "matplotlib",
        "seaborn",
        "pandas"
    ]
    
    print("üì¶ Installing required packages...")
    for package in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])
            print(f"  ‚úì {package}")
        except subprocess.CalledProcessError:
            print(f"  ‚ö†Ô∏è Failed to install {package}, attempting without version constraint...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package.split(">=")[0]])

# Install packages
install_packages()

# ## Import Libraries and Environment Setup

import numpy as np
import pandas as pd
import os
import warnings
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig, 
    pipeline, 
    TextStreamer,
    AutoConfig
)
import textwrap
import ipywidgets as widgets
from IPython.display import display, HTML, Image
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

print("\nüöÄ DeepSeek-R1-0528-Qwen3-8B Production Implementation")
print(f"üìÖ Execution Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 70)

# List available model files
print("\nüìÅ Scanning for Model Files...")
model_base_path = "/kaggle/input/deepseek-r1-0528"
if os.path.exists(model_base_path):
    for root, dirs, files in os.walk(model_base_path):
        level = root.replace(model_base_path, '').count(os.sep)
        indent = ' ' * 2 * level
        print(f"{indent}üìÇ {os.path.basename(root)}/")
        subindent = ' ' * 2 * (level + 1)
        for file in files[:3]:  # Show first 3 files per directory
            print(f"{subindent}üìÑ {file}")
else:
    print("‚ö†Ô∏è Model directory not found. Please ensure DeepSeek dataset is attached.")

# ## Section 1: Model Configuration Analysis

print("\n" + "=" * 70)
print("SECTION 1: Model Configuration Analysis")
print("=" * 70)

# Define model path
model_path = "/kaggle/input/deepseek-r1-0528/transformers/deepseek-r1-0528-qwen3-8b/1"

if os.path.exists(model_path):
    print(f"\n‚úÖ Model Path Verified: {model_path}")
    
    # Load and analyze configuration
    try:
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        
        print("\nüìä Model Architecture Details:")
        config_dict = {
            "Architecture": config.architectures[0] if hasattr(config, 'architectures') else "Unknown",
            "Hidden Size": config.hidden_size,
            "Number of Layers": config.num_hidden_layers,
            "Attention Heads": config.num_attention_heads,
            "Vocabulary Size": f"{config.vocab_size:,}",
            "Max Context Length": f"{config.max_position_embeddings:,}",
            "Model Type": config.model_type,
            "Torch Data Type": str(config.torch_dtype)
        }
        
        for key, value in config_dict.items():
            print(f"  ‚Ä¢ {key}: {value}")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading configuration: {str(e)}")
else:
    print(f"‚ùå Model path not found: {model_path}")
    print("Please ensure the DeepSeek dataset is properly attached to this notebook.")

# ## Section 2: GPU Detection and Resource Management

print("\n" + "=" * 70)
print("SECTION 2: GPU Detection and Resource Management")
print("=" * 70)

# Check CUDA availability
cuda_available = torch.cuda.is_available()
device = torch.device("cuda" if cuda_available else "cpu")

print(f"\nüñ•Ô∏è Computing Resources:")
print(f"  ‚Ä¢ PyTorch Version: {torch.__version__}")
print(f"  ‚Ä¢ CUDA Available: {cuda_available}")

if cuda_available:
    print(f"  ‚Ä¢ CUDA Version: {torch.version.cuda}")
    print(f"  ‚Ä¢ GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"  ‚Ä¢ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Clear GPU memory
    torch.cuda.empty_cache()
    print("\nüßπ GPU memory cleared")
else:
    print("  ‚Ä¢ Running on CPU (Performance will be limited)")
    print("  ‚Ä¢ For optimal performance, enable GPU in Kaggle settings")

# ## Section 3: Model Loading with 4-bit Quantization

print("\n" + "=" * 70)
print("SECTION 3: Loading Model with 4-bit Quantization")
print("=" * 70)

# Configure quantization
print("\n‚öôÔ∏è Configuring 4-bit Quantization for Memory Efficiency...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load tokenizer
print("\nüìö Loading Tokenizer...")
try:
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    print("  ‚úì Tokenizer loaded successfully")
    print(f"  ‚Ä¢ Vocabulary size: {len(tokenizer):,}")
    print(f"  ‚Ä¢ Model max length: {tokenizer.model_max_length:,}")
except Exception as e:
    print(f"  ‚ùå Error loading tokenizer: {str(e)}")
    raise

# Load model
print("\nüß† Loading Quantized Model (this will take 2-3 minutes)...")
print("  ‚Ä¢ Applying 4-bit quantization")
print("  ‚Ä¢ Memory usage: ~4GB (reduced from ~16GB)")

try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    print("\n‚úÖ Model Successfully Loaded!")
    
    # Model statistics
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  ‚Ä¢ Total parameters: {total_params/1e9:.1f}B")
    print(f"  ‚Ä¢ Quantization: 4-bit (NF4)")
    print(f"  ‚Ä¢ Device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'auto'}")
    
except Exception as e:
    print(f"  ‚ùå Error loading model: {str(e)}")
    raise

# ## Section 4: Basic Inference Demonstration

print("\n" + "=" * 70)
print("SECTION 4: Basic Inference Demonstration")
print("=" * 70)

# Create text generation pipeline
print("\nüîß Creating Text Generation Pipeline...")
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Test prompt
test_prompt = "Explain the benefits of edge computing for IoT applications in manufacturing."
messages = [{"role": "user", "content": test_prompt}]

print(f"\nüìù Test Query: {test_prompt}")
print("\nüí≠ Generating Response...")

try:
    response = generator(
        messages,
        max_new_tokens=250,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )
    
    generated_text = response[0]['generated_text'][-1]['content']
    print("\nü§ñ Model Response:")
    print("-" * 70)
    wrapped_text = textwrap.fill(generated_text, width=80)
    print(wrapped_text)
    print("-" * 70)
    
except Exception as e:
    print(f"‚ùå Error during inference: {str(e)}")

# ## Section 5: Batch Processing for Efficiency

print("\n" + "=" * 70)
print("SECTION 5: Batch Processing for Production Efficiency")
print("=" * 70)

batch_queries = [
    "What are the key advantages of microservices architecture?",
    "How can companies implement zero-trust security models?",
    "Explain the concept of data lakehouse architecture."
]

print("\nüì¶ Processing Batch of Business Queries:")
for i, query in enumerate(batch_queries, 1):
    print(f"  {i}. {query}")

batch_messages = [[{"role": "user", "content": query}] for query in batch_queries]

print("\n‚ö° Executing Batch Processing...")

try:
    batch_start_time = datetime.now()
    
    batch_responses = generator(
        batch_messages,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.6,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )
    
    batch_end_time = datetime.now()
    batch_duration = (batch_end_time - batch_start_time).total_seconds()
    
    print(f"\n‚úÖ Batch Processing Complete in {batch_duration:.2f} seconds")
    print(f"   Average time per query: {batch_duration/len(batch_queries):.2f} seconds")
    
    # Display results
    for i, (query, response) in enumerate(zip(batch_queries, batch_responses), 1):
        print(f"\n{'='*70}")
        print(f"Query {i}: {query}")
        print(f"{'='*70}")
        generated_text = response[0]['generated_text'][-1]['content']
        wrapped_text = textwrap.fill(generated_text, width=80)
        print(wrapped_text)
        
except Exception as e:
    print(f"‚ùå Error during batch processing: {str(e)}")

# ## Section 6: Structured JSON Generation

print("\n" + "=" * 70)
print("SECTION 6: Structured JSON Output Generation")
print("=" * 70)

json_task = "Create a risk assessment framework for cloud migration"

json_template = {
    "framework_name": "string",
    "assessment_categories": [
        {
            "category": "string",
            "risk_level": "low|medium|high",
            "key_risks": ["string"],
            "mitigation_strategies": ["string"]
        }
    ],
    "implementation_phases": ["string"],
    "success_metrics": ["string"]
}

json_prompt = [
    {
        "role": "system",
        "content": "You are a risk management expert. Provide responses in valid JSON format only, with no additional text or explanation."
    },
    {
        "role": "user",
        "content": f"""Create a {json_task}.

Return a JSON object following this exact structure:
{json.dumps(json_template, indent=2)}

Include realistic, actionable content for all fields."""
    }
]

print(f"üìã Task: {json_task}")
print("\nüîÑ Generating Structured JSON Response...")

try:
    json_response = generator(
        json_prompt,
        max_new_tokens=600,
        do_sample=True,
        temperature=0.3,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )
    
    raw_output = json_response[0]['generated_text'][-1]['content']
    
    # Clean markdown formatting if present
    if "```json" in raw_output:
        json_text = raw_output.split("```json")[1].split("```")[0]
    elif "```" in raw_output:
        json_text = raw_output.split("```")[1].split("```")[0]
    else:
        json_text = raw_output
    
    # Parse and display JSON
    parsed_json = json.loads(json_text.strip())
    print("\n‚úÖ Successfully Generated Structured Output:")
    print(json.dumps(parsed_json, indent=2))
    
except json.JSONDecodeError as e:
    print(f"‚ö†Ô∏è JSON parsing error: {str(e)}")
    print("\nRaw output:")
    print(raw_output[:500] + "..." if len(raw_output) > 500 else raw_output)
except Exception as e:
    print(f"‚ùå Error generating JSON: {str(e)}")

# ## Section 7: Interactive Chat Interface

print("\n" + "=" * 70)
print("SECTION 7: Interactive Chat Interface")
print("=" * 70)

# Create interactive components
query_input = widgets.Textarea(
    value="What are the best practices for implementing DevOps in enterprise environments?",
    placeholder='Enter your query...',
    description='Query:',
    layout=widgets.Layout(width='90%', height='100px')
)

temp_slider = widgets.FloatSlider(
    value=0.7,
    min=0.1,
    max=1.0,
    step=0.1,
    description='Temperature:',
    style={'description_width': 'initial'}
)

tokens_slider = widgets.IntSlider(
    value=200,
    min=50,
    max=500,
    step=50,
    description='Max Tokens:',
    style={'description_width': 'initial'}
)

generate_btn = widgets.Button(
    description="Generate Response",
    button_style='success',
    icon='rocket'
)

output_display = widgets.Output()

def process_query(btn):
    with output_display:
        output_display.clear_output()
        print("‚è≥ Processing query...")
        
        try:
            query = query_input.value
            temp = temp_slider.value
            max_tokens = tokens_slider.value
            
            messages = [{"role": "user", "content": query}]
            
            response = generator(
                messages,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=temp,
                top_p=0.95,
                pad_token_id=tokenizer.eos_token_id
            )
            
            output_display.clear_output()
            print(f"üìù Query: {query}")
            print(f"‚öôÔ∏è Settings: Temperature={temp}, Max Tokens={max_tokens}")
            print("\n" + "-" * 70)
            
            generated_text = response[0]['generated_text'][-1]['content']
            wrapped_text = textwrap.fill(generated_text, width=80)
            print(wrapped_text)
            
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")

generate_btn.on_click(process_query)

print("\nüí¨ Interactive Chat Interface Ready")
print("Adjust parameters and click 'Generate Response' to interact with the model.\n")

display(widgets.VBox([
    query_input,
    widgets.HBox([temp_slider, tokens_slider]),
    generate_btn,
    output_display
]))

# ## Section 8: Performance Benchmarking Visualization

print("\n" + "=" * 70)
print("SECTION 8: Model Performance Analysis")
print("=" * 70)

# Performance data
performance_data = {
    "Model": [
        "GPT-4",
        "Claude-3",
        "Gemini-1.5",
        "Llama-3-70B",
        "Mixtral-8x7B",
        "DeepSeek-R1-8B",
        "Qwen-2.5-72B"
    ],
    "MMLU": [86.4, 86.8, 83.7, 82.0, 70.6, 79.2, 77.9],
    "HumanEval": [85.4, 84.9, 74.4, 81.7, 40.2, 73.8, 64.6],
    "GSM8K": [92.0, 95.0, 86.5, 93.0, 74.4, 84.7, 79.6],
    "Size_B": [1760, 1750, 1500, 70, 56, 8, 72]
}

df_perf = pd.DataFrame(performance_data)

# Create visualization
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('DeepSeek-R1-8B Performance Benchmarking', fontsize=18, fontweight='bold')

# Color scheme
highlight_color = '#FF4B4B'
default_color = '#4B9BFF'
colors = [highlight_color if 'DeepSeek' in name else default_color for name in df_perf['Model']]

# Plot 1: MMLU Performance
bars1 = ax1.bar(df_perf['Model'], df_perf['MMLU'], color=colors)
ax1.set_title('MMLU (General Knowledge)', fontsize=14, fontweight='bold')
ax1.set_ylabel('Score (%)', fontsize=12)
ax1.set_ylim(60, 100)
ax1.tick_params(axis='x', rotation=45, labelsize=10)

for bar, score in zip(bars1, df_perf['MMLU']):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
             f'{score:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Plot 2: Coding Performance
bars2 = ax2.bar(df_perf['Model'], df_perf['HumanEval'], color=colors)
ax2.set_title('HumanEval (Coding)', fontsize=14, fontweight='bold')
ax2.set_ylabel('Score (%)', fontsize=12)
ax2.set_ylim(30, 100)
ax2.tick_params(axis='x', rotation=45, labelsize=10)

for bar, score in zip(bars2, df_perf['HumanEval']):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
             f'{score:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Plot 3: Math Performance
bars3 = ax3.bar(df_perf['Model'], df_perf['GSM8K'], color=colors)
ax3.set_title('GSM8K (Mathematics)', fontsize=14, fontweight='bold')
ax3.set_ylabel('Score (%)', fontsize=12)
ax3.set_ylim(60, 100)
ax3.tick_params(axis='x', rotation=45, labelsize=10)

for bar, score in zip(bars3, df_perf['GSM8K']):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
             f'{score:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Plot 4: Efficiency Metric
efficiency = df_perf['MMLU'] / (df_perf['Size_B'] / 10)
bars4 = ax4.bar(df_perf['Model'], efficiency, color=colors)
ax4.set_title('Efficiency (MMLU per 10B Parameters)', fontsize=14, fontweight='bold')
ax4.set_ylabel('Efficiency Score', fontsize=12)
ax4.tick_params(axis='x', rotation=45, labelsize=10)

for bar, eff in zip(bars4, efficiency):
    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
             f'{eff:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Add legend
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor=highlight_color, label='DeepSeek-R1-8B'),
    Patch(facecolor=default_color, label='Other Models')
]
fig.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98))

plt.tight_layout()
plt.show()

# Performance summary
print("\nüìä DeepSeek-R1-8B Performance Summary:")
print("-" * 50)
deepseek_data = df_perf[df_perf['Model'] == 'DeepSeek-R1-8B'].iloc[0]
print(f"Model Size: {deepseek_data['Size_B']}B parameters")
print(f"MMLU Score: {deepseek_data['MMLU']}% (General Knowledge)")
print(f"HumanEval: {deepseek_data['HumanEval']}% (Coding Tasks)")
print(f"GSM8K: {deepseek_data['GSM8K']}% (Mathematics)")
print(f"Efficiency: {efficiency[df_perf['Model'] == 'DeepSeek-R1-8B'].values[0]:.1f}x (Best in class)")

# ## Section 9: Production Deployment Guidelines

print("\n" + "=" * 70)
print("SECTION 9: Production Deployment Guidelines")
print("=" * 70)

deployment_guide = {
    "Infrastructure Requirements": [
        "Minimum GPU: NVIDIA T4 (16GB) with quantization",
        "Recommended GPU: NVIDIA A100 (40GB) for optimal performance",
        "RAM: 32GB minimum for model loading",
        "Storage: 20GB for model files and cache",
        "CUDA Version: 11.0 or higher"
    ],
    "Performance Optimization": [
        "Use 4-bit quantization to reduce memory by 75%",
        "Implement batch processing for multiple requests",
        "Enable GPU memory optimization with device_map='auto'",
        "Use streaming for real-time applications",
        "Cache frequently used prompts and responses"
    ],
    "Best Practices": [
        "Monitor GPU memory usage during inference",
        "Implement request queuing for high-load scenarios",
        "Use temperature 0.3-0.5 for factual tasks",
        "Use temperature 0.7-0.9 for creative tasks",
        "Validate JSON outputs with schema validation",
        "Implement timeout mechanisms for long-running requests"
    ],
    "Security Considerations": [
        "Implement input validation and sanitization",
        "Use API rate limiting to prevent abuse",
        "Enable request logging for audit trails",
        "Implement user authentication for API access",
        "Regular security updates for dependencies"
    ]
}

for category, items in deployment_guide.items():
    print(f"\nüìå {category}:")
    for item in items:
        print(f"  ‚Ä¢ {item}")

# ## Final Summary

print("\n" + "=" * 70)
print("üéâ Implementation Complete!")
print("=" * 70)

print("""
This notebook demonstrates a production-ready implementation of DeepSeek-R1-0528-8B,
showcasing its capabilities for enterprise deployment. The model offers exceptional
performance relative to its size, making it ideal for organizations seeking
advanced AI capabilities with reasonable infrastructure requirements.

Key Achievements:
‚úì Successfully loaded 8B parameter model with 4-bit quantization
‚úì Demonstrated batch processing for efficiency
‚úì Implemented structured output generation
‚úì Created interactive interface for testing
‚úì Analyzed performance benchmarks
‚úì Provided production deployment guidelines

For additional support or advanced implementations, refer to the official
DeepSeek documentation and community resources.
""")

print(f"\nüìù Total Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


def predict_handler(request):
    """HTTP Cloud Function entry point."""
    try:
        # Handle CORS
        if request.method == 'OPTIONS':
            headers = {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST',
                'Access-Control-Allow-Headers': 'Content-Type',
                'Access-Control-Max-Age': '3600'
            }
            return ('', 204, headers)

        # Set CORS headers for main request
        headers = {'Access-Control-Allow-Origin': '*'}

        if request.method == 'POST':
            request_json = request.get_json(silent=True)
            if request_json is None:
                return jsonify({'error': 'No JSON data provided'}), 400, headers
            # Use notebook-defined processing if present; otherwise call deploy_model.predict if available; else echo
            try:
                if 'process_request' in globals() and callable(globals()['process_request']):
                    result = globals()['process_request'](request_json)
                elif _HAS_MODEL and hasattr(_deploy_model, 'predict'):
                    # Minimal example: expects features in JSON under "features"
                    features = request_json.get('features')
                    if features is None:
                        return jsonify({'error': 'Missing "features" in request body'}), 400, headers
                    # This is a placeholder; real model would require proper loading
                    result = {'prediction': _deploy_model.predict(features)}
                else:
                    result = {'echo': request_json}
            except Exception as inner_e:
                logger.exception('Processing error')
                return jsonify({'error': str(inner_e)}), 500, headers
            return jsonify(result), 200, headers
        elif request.method == 'GET':
            return jsonify({'status': 'Kaggle notebook API is running'}), 200, headers
        else:
            return jsonify({'error': 'Method not allowed'}), 405, headers
    except Exception as e:
        logger.error(f"Error processing request: {e}")
        return jsonify({'error': str(e)}), 500, headers

# For local testing
if __name__ == '__main__':
    app = Flask(__name__)
    app.add_url_rule('/', 'predict_handler', predict_handler, methods=['GET', 'POST'])
    app.run(debug=True, port=8080)
