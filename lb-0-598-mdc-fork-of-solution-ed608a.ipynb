{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82370,"databundleVersionId":13015230,"sourceType":"competition"},{"sourceId":248118764,"sourceType":"kernelVersion"},{"sourceId":166368,"sourceType":"modelInstanceVersion","modelInstanceId":141565,"modelId":164048}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":474.866415,"end_time":"2025-07-30T12:51:20.001095","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-30T12:43:25.13468","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! uv pip uninstall --system 'tensorflow'\n! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n! mkdir -p /tmp/src","metadata":{"_uuid":"1aab990f-6851-4bee-8a61-0d291289eb76","_cell_guid":"e60b5615-7b26-4e33-a68c-208a6c801557","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T03:49:06.376608Z","iopub.execute_input":"2025-08-01T03:49:06.376918Z","iopub.status.idle":"2025-08-01T03:49:29.373634Z","shell.execute_reply.started":"2025-08-01T03:49:06.376898Z","shell.execute_reply":"2025-08-01T03:49:29.372761Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":20.083729,"end_time":"2025-07-30T12:43:50.835737","exception":false,"start_time":"2025-07-30T12:43:30.752008","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /tmp/src/helpers.py\nimport logging, os, kagglehub, inspect\nfrom pathlib import Path\nimport polars as pl\n\nIS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\nIS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\nCOMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\nPDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\nWORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n\nDOI_LINK = 'https://doi.org/'\n\nDEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\nLOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\nLOG_DIR = Path(LOG_FILE_PATH).parent\n\nLOG_DIR.mkdir(parents=True, exist_ok=True)\n\nLOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\nLOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n\ndef get_logger(name=None):\n    if name is None:\n        frame = inspect.currentframe()\n        if frame is None or frame.f_back is None:\n            name = \"__main__\"\n        else:\n            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n\n    logger = logging.getLogger(name)\n\n    if not logger.handlers:\n        logger.setLevel(DEFAULT_LOG_LEVEL)\n        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n        ch = logging.StreamHandler()\n        ch.setLevel(DEFAULT_LOG_LEVEL)\n        ch.setFormatter(formatter)\n        fh = logging.FileHandler(LOG_FILE_PATH)\n        fh.setLevel(DEFAULT_LOG_LEVEL)\n        fh.setFormatter(formatter)\n        logger.addHandler(ch)\n        logger.addHandler(fh)\n        logger.propagate = False\n    return logger\n\ndef is_doi_link(name: str) -> pl.Expr:\n    return pl.col(name).str.starts_with(DOI_LINK)\n\ndef string_normalization(name: str) -> pl.Expr:\n    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n\ndef get_df(parse_dir: str):\n    records = []\n    txt_files = list(Path(parse_dir).glob('*.txt'))\n    for txt_file in txt_files:\n        id_ = txt_file.stem\n        with open(txt_file, 'r') as f:\n            text = f.read()\n        records.append({'article_id': id_, 'text': text})\n    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n\ndef assume_type(df: pl.DataFrame) -> pl.DataFrame:\n    return (\n        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n    )\n\ndef score(df, gt, on, tag='all'):\n    hits = gt.join(df, on=on)\n    tp = hits.height\n    fp = df.height - tp\n    fn = gt.height - tp\n    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n\ndef evaluate(df, on=['article_id', 'dataset_id']):\n    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n    return (\n        score(df, gt, on),\n        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n    )","metadata":{"_uuid":"f626c2c1-4a91-445a-bb8d-0d4f90044e20","_cell_guid":"1cb75f66-df5b-4cb0-bee9-74795dc9d398","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T03:49:29.375316Z","iopub.execute_input":"2025-08-01T03:49:29.375596Z","iopub.status.idle":"2025-08-01T03:49:29.383472Z","shell.execute_reply.started":"2025-08-01T03:49:29.375571Z","shell.execute_reply":"2025-08-01T03:49:29.382574Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.018527,"end_time":"2025-07-30T12:43:50.864121","exception":false,"start_time":"2025-07-30T12:43:50.845594","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /tmp/src/parse.py\nimport argparse\nfrom pathlib import Path\nimport pymupdf\nfrom helpers import get_logger, PDF_DIR\n\nl = get_logger()\n\ndef pdf_to_txt(output_dir: Path):\n    output_dir.mkdir(parents=True, exist_ok=True)\n    pdf_files = list(PDF_DIR.glob(\"*.pdf\")) + list(PDF_DIR.glob(\"*.PDF\"))\n    existing_txt_files = {f.stem for f in output_dir.glob(\"*.txt\")}\n    for pdf_file in pdf_files:\n        txt_file = output_dir / f\"{pdf_file.stem}.txt\"\n        if pdf_file.stem in existing_txt_files:\n            continue\n        try:\n            text = \"\"\n            with pymupdf.open(pdf_file) as doc:\n                for page in doc:\n                    text += page.get_text()\n            txt_file.write_text(text, encoding='utf-8')\n        except Exception:\n            pass\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('output_dir', type=Path, help='Directory to save text files')\n    args = parser.parse_args()\n    pdf_to_txt(args.output_dir)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"4cb32e3e-fc8f-40a9-8316-36ff0a011fd0","_cell_guid":"004db869-9ca8-46c7-85e5-d46868f3b68d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T03:49:29.384265Z","iopub.execute_input":"2025-08-01T03:49:29.384598Z","iopub.status.idle":"2025-08-01T03:49:31.041735Z","shell.execute_reply.started":"2025-08-01T03:49:29.38457Z","shell.execute_reply":"2025-08-01T03:49:31.040864Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.014728,"end_time":"2025-07-30T12:43:50.888086","exception":false,"start_time":"2025-07-30T12:43:50.873358","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /tmp/src/check_parse.py\nimport polars as pl\nfrom pathlib import Path\nfrom helpers import *\n\nl=get_logger()\n\ndef gt_dataset_id_normalization(name:str) -> pl.Expr:\n    return (\n        pl.when(is_doi_link(name))\n        .then(pl.col(name).str.split(DOI_LINK).list.last())\n        .otherwise(name)\n        .str.to_lowercase()\n    )\n\ndef main():\n    if IS_KAGGLE_SUBMISSION:\n        l.debug('skipping check_parse for submission')\n        return\n    df = (\n        get_df('/tmp/train_parse')\n        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n    )\n\n    gt = (\n        pl.read_csv(COMP_DIR/'train_labels.csv')\n        .filter(pl.col('article_id').is_in(df['article_id']))\n        .filter(pl.col('type')!='Missing')\n        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n    )\n\n    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n\nif __name__=='__main__': main()","metadata":{"_uuid":"cf79119b-0dd3-4ca9-8220-2863dcafaff8","_cell_guid":"9b36aabf-2e98-448d-83bb-768c4667881a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T03:49:31.043614Z","iopub.execute_input":"2025-08-01T03:49:31.043846Z","iopub.status.idle":"2025-08-01T03:49:31.056043Z","shell.execute_reply.started":"2025-08-01T03:49:31.043828Z","shell.execute_reply":"2025-08-01T03:49:31.055301Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016232,"end_time":"2025-07-30T12:43:50.913072","exception":false,"start_time":"2025-07-30T12:43:50.89684","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /tmp/src/getid.py\nimport re\nimport polars as pl\nfrom typing import Optional, Tuple\n\nfrom helpers import *\n\nCOMPILED_PATTERNS = {\n    'ref_header_patterns': [re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS|ACKNOWLEDGEMENTS)\\b[:\\s]*', re.IGNORECASE)],    \n    'citation_pattern': re.compile(r'^\\s*(\\[\\d+\\]|\\(\\d+\\)|\\d+\\.|\\d+\\)|\\d+(?=\\s|$))\\s*'),\n    'first_citation_patterns': [\n        re.compile(r'^\\s*\\[1\\]\\s*'),\n        re.compile(r'^\\s*\\(1\\)\\s*'),\n        re.compile(r'^\\s*1\\.\\s*'),\n        re.compile(r'^\\s*1\\)\\s*'),\n        re.compile(r'^\\s*1(?=\\s|$)'),\n    ],\n}\n\nl = get_logger()\n\ndef find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n    last_match_idx = None\n    for pattern in header_patterns:\n        matches = list(pattern.finditer(text))\n        if matches:\n            last_match_idx = matches[-1].start()\n    return last_match_idx\n\ndef find_last_first_citation(text: str) -> Optional[int]:\n    lines = text.splitlines()\n    last_match_line = None\n    for line_num, line in enumerate(lines):\n        line = line.strip()\n        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n            if pattern.match(line):\n                next_lines = lines[line_num:line_num+3]\n                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n                    last_match_line = line_num\n                break\n    return last_match_line\n\ndef find_reference_start(text: str) -> Optional[int]:\n    lines = text.splitlines()\n    last_first_citation = find_last_first_citation(text)\n    if last_first_citation is not None:\n        return last_first_citation\n    start_search_idx = int(len(lines) * 0.5)\n    for i in range(start_search_idx, len(lines)):\n        line = lines[i].strip()\n        if COMPILED_PATTERNS['citation_pattern'].match(line):\n            next_lines = lines[i:i+3]\n            if sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip())) >= 2:\n                for j in range(i, max(-1, i-10), -1):\n                    if not COMPILED_PATTERNS['citation_pattern'].match(lines[j].strip()):\n                        return j + 1\n                return max(0, i-10)\n    return None\n\ndef split_text_and_references(text: str) -> Tuple[str, str]:\n    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n    if header_idx is not None:\n        header_idx2 = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n        if header_idx2 is not None:\n            header_idx3 = find_last_reference_header(text[:header_idx2].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n            if header_idx3 is not None:\n                return text[:header_idx3].strip(), text[header_idx3:].strip()\n            return text[:header_idx2].strip(), text[header_idx2:].strip()\n        return text[:header_idx].strip(), text[header_idx:].strip()\n    ref_start_line = find_reference_start(text)\n    if ref_start_line is not None:\n        lines = text.splitlines()\n        body = '\\n'.join(lines[:ref_start_line])\n        refs = '\\n'.join(lines[ref_start_line:])\n        return body.strip(), refs.strip()\n    return text.strip(), ''\n\ndef get_splits(df: pl.DataFrame) -> pl.DataFrame:\n    bodies, refs = [], []\n    for raw_text in df['text']:\n        main, ref = split_text_and_references(raw_text)\n        bodies.append(main)\n        refs.append(ref)\n    return df.with_columns(pl.Series('body', bodies), pl.Series('ref', refs))\n\ndef tidy_extraction(df) -> pl.DataFrame:\n    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n\n    doi_df = (\n        df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n          .explode('match')\n          .drop_nulls('match')\n          .with_columns(\n              pl.col('match').str.replace_all(r'\\s+', '')\n                             .str.replace(r'[^A-Za-z0-9]+$', '')\n                             .str.to_lowercase()\n                             .alias('dataset_id')\n          )\n          .group_by('article_id', 'dataset_id')\n          .agg('match')\n          .with_columns((DOI_LINK + pl.col('dataset_id')).alias('dataset_id'))\n    )\n\n    # REGEX_IDS = (\n    #     r\"(?i)\\b(?:\"\n    #     r\"CHEMBL\\d+|\"\n    #     r\"E-GEOD-\\d+|E-PROT-\\d+|E-MTAB-\\d+|E-MEXP-\\d+|EMPIAR-\\d+|\"\n    #     r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n    #     r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n    #     r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|BX\\d{6}|KX\\d{6}|K0\\d{4}|CAB\\d{6}|\"\n    #     r\"NC_\\d{6}\\.\\d{1}|NM_\\d{9}|\"\n    #     r\"PRJNA\\d+|PRJEB\\d+|PRJDB\\d+|PXD\\d+|SAMN\\d+|\"\n    #     r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n    #     r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n    #     r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n    #     r\"(?:SR[PRX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+\"\n    #     r\")\"\n    # )  \n\n    REGEX_IDS = (\n        r\"(?i)\\b(?:\"\n        r\"CHEMBL\\d+|\"\n        r\"E-GEOD-\\d+|E-PROT-\\d+|E-MTAB-\\d+|E-MEXP-\\d+|EMPIAR-\\d+|\"\n        r\"ENSBTAG\\d+|ENSOARG\\d+|\"\n        r\"EPI_ISL_\\d{5,}|EPI\\d{6,7}|\"\n        r\"HPA\\d+|CP\\d{6}|IPR\\d{6}|PF\\d{5}|BX\\d{6}|KX\\d{6}|K0\\d{4}|CAB\\d{6}|\"\n        r\"NC_\\d{6}\\.\\d{1}|NM_\\d{9}|\"\n        # 修正点1: PRJEB を追加\n        r\"PRJNA\\d+|PRJEB\\d+|PRJDB\\d+|PXD\\d+|SAMN\\d+|\"\n        r\"GSE\\d+|GSM\\d+|GPL\\d+|\"\n        r\"PDB\\s?[1-9][A-Z0-9]{3}|HMDB\\d+|\"\n        r\"dryad\\.[^\\s\\\"<>]+|pasta\\/[^\\s\\\"<>]+|\"\n        # 修正点2: SRR と SRA にもマッチするように SR[PX] を SR[RPAX] へ変更\n        r\"(?:SR[RPAX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+|\"\n        r\"CVCL_[A-Z0-9]{4}\"\n        r\")\"\n    )\n    \n    acc_df = (\n        df.with_columns(\n            pl.col('text').str.extract_all(REGEX_IDS).alias('match')\n        )\n        .explode('match')\n        .drop_nulls('match')\n        .with_columns(\n            pl.col('match').str.replace_all(r'\\s+', '')\n                           .str.replace(r'[^A-Za-z0-9]+$', '')\n                           .str.replace(r'(?i)^PDB', '')\n                           .alias('dataset_id')\n        )\n        .group_by('article_id', 'dataset_id')\n        .agg('match')\n        .with_columns(\n            pl.when(pl.col('dataset_id').str.starts_with('dryad.'))\n              .then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id'))\n              .otherwise('dataset_id')\n              .alias('dataset_id')\n        )\n        .with_columns(\n            pl.when(pl.col('dataset_id').str.starts_with('pasta/'))\n              .then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id'))\n              .otherwise('dataset_id')\n              .alias('dataset_id')\n        )\n    )\n\n    df = pl.concat([doi_df, acc_df])\n\n    df = (\n        df.unique(['article_id', 'dataset_id'])  # CHANGED\n          .filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex()))\n          .filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex()))\n          .filter(~pl.col('dataset_id').str.contains('figshare', literal=True))\n          .filter(~pl.col('dataset_id').is_in(bad_ids))\n          .filter(\n              pl.when(is_doi_link('dataset_id') &\n                      (pl.col('dataset_id').str.split('/').list.last().str.len_chars() < 5))\n               .then(False)\n               .otherwise(True)\n          )\n          .with_columns(pl.col('match').list.unique())\n    )\n    return df\n\ndef get_context_window(text: str, substring: str, window: int = 100) -> str:\n    idx = text.find(substring)\n    if idx == -1:\n        raise ValueError\n    start = max(idx - window, 0)\n    end = min(idx + len(substring) + window, len(text))\n    return text[start:end]\n\ndef get_window_df(text_df, ids_df):\n    df = ids_df.join(text_df, on='article_id')\n    windows = []\n    for text, match_ids in df.select('text', 'match').rows():\n        windows.append(get_context_window(text, match_ids[0]))\n    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n\ndef main():\n    text_df = get_df('/tmp/train_parse')\n    df = get_splits(text_df)\n    df = tidy_extraction(df)\n    df = get_window_df(text_df, df)\n    df.write_parquet('/tmp/extracted.parquet')\n    df = assume_type(df)\n    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n    if not IS_KAGGLE_SUBMISSION:\n        results = evaluate(df)\n        for r in results: l.info(r)\n        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n        for r in results: l.info(r)\n\nif __name__=='__main__': main()","metadata":{"_uuid":"35aa1a1d-cd2d-48bc-aa71-26d443f3a74d","_cell_guid":"5e11976c-9396-4beb-88fd-9e70cc409690","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T03:49:31.056944Z","iopub.execute_input":"2025-08-01T03:49:31.057175Z","iopub.status.idle":"2025-08-01T03:49:31.070338Z","shell.execute_reply.started":"2025-08-01T03:49:31.057148Z","shell.execute_reply":"2025-08-01T03:49:31.069697Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.019717,"end_time":"2025-07-30T12:43:50.939863","exception":false,"start_time":"2025-07-30T12:43:50.920146","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /tmp/src/llm_validate.py\nimport polars as pl\nimport os\n\nfrom helpers import *\n\nl = get_logger()\n\nSYS_PROMPT_CLASSIFY_DOI = \"\"\"\nROLE\nYou are a validator that decides if the TARGET DOI refers to a DATA REPOSITORY / DATASET (label A)\nor to LITERATURE / NON-DATA resource (label B), using only the given local context.\n\nDECISION ORDER (apply top-down; stop at the first satisfied rule)\n\n1) STRONG DATA → A\n   Trigger if ANY is true:\n   • TARGET DOI has a common data-repository prefix or path:\n     10.5061 (Dryad), 10.5281 (Zenodo), 10.6084 (Figshare), 10.17632 (Mendeley Data),\n     10.7910/DVN (Dataverse), 10.18112/openneuro., 10.1594/PANGAEA., 10.21233,\n     10.3886 (ICPSR), 10.7289 (NOAA), 10.5255 (UKDS), 10.6019 (EMPIAR)\n     Path hints: “m9.figshare.”, “zenodo.”, “dryad.”\n   • Within ±120 characters around the TARGET DOI, context includes clear data cues:\n     data availability, dataset(s), data repository, data archive, deposited, available at,\n     hosted by, stored on, accession, retrieved from, provided by.\n\n2) STRONG LITERATURE → B\n   Trigger if ANY is true (near the TARGET DOI):\n   • Bibliographic formatting: author list + year; journal/proceedings/book name; volume/issue/pages;\n     cues like “In:”, “pp.”, “vol.”, “no.”, “et al.”, “(20xx)”.\n   • Phrases indicating papers rather than datasets: “this article/paper”, “review”, “preprint”,\n     “protocol”, “method”, “editorial”.\n   • Publisher DOI prefixes (not repositories): 10.1038, 10.1007, 10.1126, 10.1016, 10.1101, 10.1021,\n     10.1093, 10.1080, 10.1111, 10.1039, 10.1002, 10.3390, 10.1073, 10.1103, 10.1186, 10.1371, 10.7554.\n   • “supplementary material/information” hosted on a publisher site (no repository mentioned).\n   • Mentions of code/software/tools/packages without “data/dataset/repository”.\n\n3) CONFLICTS (both data & literature cues present)\n   Prefer A only if the TARGET DOI itself is a repository DOI OR the nearby text explicitly says\n   the dataset is available at that TARGET DOI. Otherwise B.\n\n4) UNCERTAIN → B\n   If evidence is unclear or insufficient, default to B.\n\nOUTPUT FORMAT\nReturn exactly one character: A or B (no other text, no punctuation).\n\nFEW-SHOT EXAMPLES\n• “Data are available at Dryad (DOI 10.5061/dryad.ab123cd).” → A\n• “Raw images uploaded to Figshare, see 10.6084/m9.figshare.1234567.” → A\n• “Sequence reads are deposited under accession PRJNA123456; see also DOI 10.5281/zenodo.9999999.” → A\n• “As described in Nature (DOI 10.1038/s41586-020-00000).” → B\n• “Smith et al. (2021) J Neurosci 41:123–130. doi:10.1523/JNEUROSCI.…” → B\n• “See Supplementary Information (doi:10.1038/s41467-…).” → B\n• “Data Availability: datasets are hosted by Zenodo (10.5281/zenodo.12345).” → A\n• “This article is published in Science (DOI 10.1126/science.abc1234).” → B\n\"\"\".strip()\n\ndef build_df():\n    df = pl.read_parquet('/tmp/extracted.parquet')\n    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n    return df.filter(is_doi_link('dataset_id'))\n\ndef build_prompt(tokenizer, df):\n    prompts = []\n    for doi, text in df.select('dataset_id', 'window').rows():\n        user = f\"TARGET DOI: {doi}\\n\\nLocal context where it appears:\\n{text or ''}\\n\\nAnswer with A or B.\"\n        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI},\n                    {'role':'user', 'content': user}]\n        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False))\n    return df.with_columns(pl.Series('prompt', prompts))\n\nif __name__=='__main__':\n    os.environ[\"VLLM_USE_V1\"] = \"0\"\n    import vllm\n    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n    llm = vllm.LLM(model_path, quantization='awq', tensor_parallel_size=2, gpu_memory_utilization=0.9, trust_remote_code=True, dtype=\"half\", enforce_eager=True, max_model_len=2048, disable_log_stats=True, disable_custom_all_reduce=True, enable_prefix_caching=True, task='generate')\n    tokenizer = llm.get_tokenizer()\n    df = build_df()\n    df = build_prompt(tokenizer, df)\n    prompts = df['prompt'].to_list()\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n    choices = [max(d, key=d.get) for d in logprobs]\n    types = {'A': True, 'B': False}\n    choices = [types[c] for c in choices]\n    df = df.with_columns(pl.Series('type', choices))\n    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n    df = assume_type(df)\n    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n    if not IS_KAGGLE_SUBMISSION:\n        results = evaluate(df)\n        for r in results: l.info(r) \n        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n        for r in results: l.info(r)","metadata":{"_uuid":"204ca515-6c62-4299-834e-0f2026d0fca1","_cell_guid":"d2077800-bcc4-46f1-9365-fb61b80bc2ed","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T03:49:31.071193Z","iopub.execute_input":"2025-08-01T03:49:31.071381Z","iopub.status.idle":"2025-08-01T03:49:31.085361Z","shell.execute_reply.started":"2025-08-01T03:49:31.071365Z","shell.execute_reply":"2025-08-01T03:49:31.084469Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.019658,"end_time":"2025-07-30T12:43:50.971551","exception":false,"start_time":"2025-07-30T12:43:50.951893","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /tmp/src/post_filter.py\nimport polars as pl\nfrom helpers import *\n\n\"\"\"\nFourth essence: Post-filter to cut FP DOIs that look like literature.\n- Read /kaggle/working/submission.csv (output of llm_validate.py)\n- Join with /tmp/extracted.parquet to get context window\n- Drop DOI rows that (1) start with typical publisher prefixes AND (2) have no data-ish words nearby\n- Keep accessions untouched\n\"\"\"\n\nl = get_logger()\n\n# 修正点3: 一般的な学術出版社のプレフィックスをリストに追加\nPAPER_PREFIXES = [\n    \"10.1038\",\"10.1007\",\"10.1126\",\"10.1016\",\"10.1101\",\"10.1021\",\"10.1145\",\"10.1177\",\n    \"10.1093\",\"10.1080\",\"10.1111\",\"10.1098\",\"10.1103\",\"10.1186\",\"10.1371\",\"10.7554\",\n    \"10.1039\",\"10.1002\",\"10.3390\",\"10.1073\",\"10.1097\",\"10.15252\",\"10.1136\",\"10.1091\",\n    \"10.1523\", \"10.1152\", \"10.1128\", \"10.1155\", \"10.1242\", \"10.1182\", \"10.1012\"\n]\n\n# 修正点4: データを示唆するキーワードの正規表現を強化\nCONTEXT_RE = r\"(?i)\\b(data(?: ?set)?|database|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession(?: number| code)?|files|retrieved from)\\b\"\n\ndef is_paper_prefix(col: str = \"dataset_id\") -> pl.Expr:\n    expr = pl.lit(False)\n    for p in PAPER_PREFIXES:\n        expr = expr | pl.col(col).str.starts_with(f\"{DOI_LINK}{p}\")\n    return expr\n\ndef main():\n    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\n\n    # Normalize columns: drop row_id if present so concat widths match\n    if \"row_id\" in sub.columns:\n        sub = sub.drop(\"row_id\")\n\n    # Context windows\n    win = pl.read_parquet(\"/tmp/extracted.parquet\").select(\"article_id\", \"dataset_id\", \"window\")\n\n    # DOI & ACC split\n    doi_rows = sub.filter(is_doi_link(\"dataset_id\")).join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n    acc_rows = sub.filter(~is_doi_link(\"dataset_id\"))\n\n    keep_mask = (\n        (~is_paper_prefix(\"dataset_id\"))  # not a known paper prefix\n        | doi_rows[\"window\"].fill_null(\"\").str.contains(CONTEXT_RE)\n    )\n\n    kept_doi = doi_rows.filter(keep_mask).select(\"article_id\", \"dataset_id\", \"type\")\n    final = pl.concat([kept_doi, acc_rows.select(\"article_id\", \"dataset_id\", \"type\")])\n\n    # Re-eval & save\n    if not IS_KAGGLE_SUBMISSION:\n        for r in evaluate(final): l.info(r)\n        for r in evaluate(final, on=[\"article_id\", \"dataset_id\", \"type\"]): l.info(r)\n\n    final.with_row_index(\"row_id\").write_csv(\"/kaggle/working/submission.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"53582a48-2b37-4e55-9066-2c64c5f206b7","_cell_guid":"e9bab64b-c820-4b8a-8c4a-bdbaf5cf617f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-01T03:49:31.086302Z","iopub.execute_input":"2025-08-01T03:49:31.086528Z","iopub.status.idle":"2025-08-01T03:49:31.097894Z","shell.execute_reply.started":"2025-08-01T03:49:31.086488Z","shell.execute_reply":"2025-08-01T03:49:31.097246Z"},"papermill":{"duration":0.018508,"end_time":"2025-07-30T12:43:51.002131","exception":false,"start_time":"2025-07-30T12:43:50.983623","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /tmp\n!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n! python src/check_parse.py\n! python src/getid.py\n! python src/llm_validate.py\n! python src/post_filter.py\n! grep \"f1:\" /tmp/logs/project.log","metadata":{"_uuid":"84b5f834-323a-4436-8bac-66a37ab9996d","_cell_guid":"41900a24-d1aa-4f94-b699-98ff2edb9084","trusted":true,"collapsed":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-08-01T03:49:31.098697Z","iopub.execute_input":"2025-08-01T03:49:31.098966Z","execution_failed":"2025-08-01T03:54:18.903Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":448.641642,"end_time":"2025-07-30T12:51:19.65281","exception":false,"start_time":"2025-07-30T12:43:51.011168","status":"completed"},"scrolled":true,"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"204c2374-90c9-48fd-8737-84e29ca5b6ae","_cell_guid":"65d29eb0-57c8-4dab-a949-32059e009e07","trusted":true,"collapsed":false,"papermill":{"duration":0.009673,"end_time":"2025-07-30T12:51:19.672901","exception":false,"start_time":"2025-07-30T12:51:19.663228","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}